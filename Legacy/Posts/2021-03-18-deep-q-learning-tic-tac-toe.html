<!DOCTYPE html>
<html lang="en">
  




<head>
	<meta charset="utf-8">
	<title>Deep Q Learning for Tic Tac Toe - Tropical Insight</title>
	<link rel="canonical" href="http://localhost:4000/Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html">
	<meta name="description" content="Inspired by Deep Mind's astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe. How hard could it be?">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:4000"},
  "headline": "Deep Q Learning for Tic Tac Toe",
  "abstract": "Inspired by Deep Mind's astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe. How hard could it be?",
    "keywords": "machine learning, artificial intelligence, reinforcement learning, coding, python",
    "wordcount": "2610",
    "image": ["http://localhost:4000/assets/imgposts/20210318/TicTacToeSml.jpg"],
  "datePublished": "2021-03-18 15:14:20 -0600",
  "dateModified": "2021-03-18 15:14:20 -0600",
  "author": {
    "@type": "Person",
    "name": "Armando Maynez"},
  "publisher": {
    "@type":  "Organization",
    "logo": {
        "@type": "ImageObject",
        "encodingFormat": "image/png",
        "contentUrl": "http://localhost:4000/assets/img/branding/logo.png",
        "url": "http://localhost:4000/assets/img/branding/logo.png"},
    "name" : "Tropical Insight"}
}
</script>
<!-- Open Graph data -->
<meta property="og:url" content="http://localhost:4000/Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html"/>
<meta property="og:type" content="article"/>
<meta property="og:title" content="Deep Q Learning for Tic Tac Toe"/>
<meta property="og:description" content="Inspired by Deep Mind's astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe. How hard could it be?"/>
<meta property="og:image" content="http://localhost:4000/assets/imgposts/20210318/TicTacToeSml.jpg"/>
<meta property="og:image:alt" content="Deep Q Learning for Tic Tac Toe"/>
<meta property="og:site_name" content="Tropical Insight" />
<meta property="article:published_time" content="2021-03-18 15:14:20 -0600" />
<meta property="article:modified_time" content="2021-03-18 15:14:20 -0600" />
<meta property="article:tag" content="machine learning, artificial intelligence, reinforcement learning, coding, python" />
<meta property="fb:admins" content="" />
<!-- Schema.org markup for Google -->
<meta itemprop="name" content="Deep Q Learning for Tic Tac Toe">
<meta itemprop="description" content="Inspired by Deep Mind's astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe. How hard could it be?">
<meta itemprop="image" content="http://localhost:4000/assets/imgposts/20210318/TicTacToeSml.jpg">
<!-- Twitter Card data -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="">
<meta name="twitter:title" content="Deep Q Learning for Tic Tac Toe">
<meta name="twitter:description" content="Inspired by Deep Mind's astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the ...">
<meta name="twitter:creator" content="">
<meta data-rh="true" name="twitter:label1" content="Word count"/>
<meta data-rh="true" name="twitter:data1" content="2610"/>
<meta name="twitter:image:src" content="http://localhost:4000/assets/img/posts/20210318/TicTacToeSml.jpg">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#311e3e">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#311e3e">
	<!-- Google Fonts -->
	<link rel="preconnect" href="https://fonts.gstatic.com" />
	<style>
/* latin */
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/lora/v17/0QIvMX1D_JOuMwr7I_FMl_E.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin */
@font-face {
  font-family: 'Lora';
  font-style: normal;
  font-weight: 600;
  src: url(https://fonts.gstatic.com/s/lora/v17/0QIvMX1D_JOuMwr7I_FMl_E.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin */
@font-face {
  font-family: 'Source Sans Pro';
  font-style: normal;
  font-weight: 200;
  src: url(https://fonts.gstatic.com/s/sourcesanspro/v14/6xKydSBYKcSV-LCoeQqfX1RYOo3i94_wlxdu3cOWxw.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin */
@font-face {
  font-family: 'Source Sans Pro';
  font-style: normal;
  font-weight: 400;
  src: url(https://fonts.gstatic.com/s/sourcesanspro/v14/6xK3dSBYKcSV-LCoeQqfX1RYOo3qOK7lujVj9w.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin */
@font-face {
  font-family: 'Source Sans Pro';
  font-style: normal;
  font-weight: 700;
  src: url(https://fonts.gstatic.com/s/sourcesanspro/v14/6xKydSBYKcSV-LCoeQqfX1RYOo3ig4vwlxdu3cOWxw.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
	</style>
	<!-- <link href="https://fonts.googleapis.com/css?family=Lora:400,600|Source+Sans+Pro:200,400,700" rel="stylesheet"> -->
	<!-- Font Awesome -->
	<link rel="stylesheet" href="../../assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="../../assets/css/main.css">
	




<link rel="icon" href="../../assets/img/favicon/favicon.ico" type="image/x-icon">
<link rel="apple-touch-icon" href="../../assets/img/favicon/favicon.ico">
<link rel="apple-touch-icon" sizes="72x72" href="../../assets/img/favicon/favicon.ico">
<link rel="apple-touch-icon" sizes="114x114" href="../../assets/img/favicon/favicon.ico">
	
	<link rel="stylesheet" href="../../assets/css/highlighter/syntax-base16.monokai.dark.css">
	
</head>

  <body>
    




<section class="hidden">
  <div class="post">
      <a  class="post-list-title" href="../../Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html">Deep Q Learning for Tic Tac Toe</a>
      

  <span class = "post-card-meta">
  
  
    <span class="meta-pre"></span>
  
  
    
      
      <span class="page_meta-date">
        <time datetime="2021-03-18T17:14:20-04:00">March 18, 2021</time>
      </span>
    
    
      <span class="meta-sep"></span>
    
  
  
    
    
    <span class="page_meta-readtime">
      
        13 minute read
      
    </span>
  
  
  </span>

        <div class="post-excerpt">
            <center><img style="float: left;margin-right: 1em;" src="./assets/img/posts/20210318/Game_Screen.png" width="310" height="300" /></center><h2 id="background">Background</h2><p>After many years of a corporate career (17) diverging from computer science, I have now decided to learn Machine Learning and in the process return to coding (something I have always loved!).</p><p>To fully grasp the essence of ML I decided to start by <a href="./ML-Library-from-scratch.html">coding a ML library myself</a>, so I can fully understand the inner workings, linear algebra and calculus involved in Stochastic Gradient Descent. And on top learn Python (I used to code in C++ 20 years ago).</p><p>I built a general purpose basic ML library that creates a Neural Network...<a class="read-more" href="../../Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html"> read more</a>
        </div>
  </div>
</section>
<div class="flex-container transparent">
  




<header class="main-header">
  <div class="wrapper">
    <div class="header-flex">
      <div class="menu-icon-container">
        <span class="menu-icon"><i class="fa fa-bars" aria-hidden="true"></i></span>
      </div>
      <nav class="main-nav">
        <span class="menu-icon-close"><i class="fa fa-times" aria-hidden="true"></i></span>
        <ul>
          <li>
            <div class="theme-toggle night">
    <input class="night" type="checkbox" id="theme-switch">
    <label class="night" for="theme-switch">
        <div class="toggle night"></div>
        <div class="names night">             
        <p class="light night"><svg class="night" width="20" viewBox="0 0 25 20" fill="currentColor" xmlns="http://www.w3.org/2000/svg">
            <path class="night" d="M12.5 2.49871C11.3401 2.50016 10.2282 2.96156 9.40801 3.78171C8.58785 4.60187 8.12645 5.71383 8.125 6.87371C8.125 7.03947 8.19085 7.19844 8.30806 7.31565C8.42527 7.43286 8.58424 7.49871 8.75 7.49871C8.91576 7.49871 9.07473 7.43286 9.19194 7.31565C9.30915 7.19844 9.375 7.03947 9.375 6.87371C9.37593 6.04519 9.70547 5.25088 10.2913 4.66503C10.8772 4.07918 11.6715 3.74964 12.5 3.74871C12.6658 3.74871 12.8247 3.68286 12.9419 3.56565C13.0592 3.44844 13.125 3.28947 13.125 3.12371C13.125 2.95795 13.0592 2.79898 12.9419 2.68177C12.8247 2.56456 12.6658 2.49871 12.5 2.49871V2.49871ZM12.5 -0.00129131C8.47891 -0.00129131 5.62031 3.26238 5.625 6.88269C5.62487 8.54403 6.22974 10.1486 7.32656 11.3964C8.32891 12.5378 9.29062 14.4007 9.375 14.9987L9.37734 17.9358C9.37744 18.0587 9.41402 18.1787 9.48242 18.2807L10.4395 19.7198C10.4964 19.8055 10.5737 19.8758 10.6644 19.9245C10.7551 19.9731 10.8564 19.9986 10.9594 19.9987H14.0395C14.1426 19.9988 14.2441 19.9734 14.3351 19.9247C14.426 19.8761 14.5034 19.8057 14.5605 19.7198L15.5176 18.28C15.5854 18.1776 15.6219 18.0578 15.6227 17.935L15.625 14.9987C15.7129 14.3846 16.6797 12.5303 17.6734 11.3964C18.5434 10.4028 19.1087 9.17963 19.3015 7.87318C19.4944 6.56673 19.3066 5.23238 18.7608 4.02985C18.215 2.82732 17.3342 1.80757 16.2238 1.09264C15.1135 0.377721 13.8206 -0.00207746 12.5 -0.00129131V-0.00129131ZM14.3727 17.7452L13.7047 18.7487H11.2937L10.6273 17.7452V17.4987H14.3738L14.3727 17.7452ZM14.375 16.2487H10.625L10.6227 14.9987H14.375V16.2487ZM16.7348 10.5725C16.1879 11.1956 15.316 12.4511 14.7594 13.7479H10.243C9.68516 12.4507 8.81328 11.1956 8.26641 10.5725C7.36971 9.5491 6.87599 8.2344 6.87734 6.87371C6.87031 3.8659 9.23594 1.24871 12.5 1.24871C15.602 1.24871 18.125 3.77176 18.125 6.87371C18.1249 8.23456 17.6305 9.54904 16.7336 10.5725H16.7348ZM3.75 6.87371C3.75 6.70795 3.68415 6.54898 3.56694 6.43177C3.44973 6.31456 3.29076 6.24871 3.125 6.24871H0.625C0.45924 6.24871 0.300269 6.31456 0.183058 6.43177C0.065848 6.54898 0 6.70795 0 6.87371C0 7.03947 0.065848 7.19844 0.183058 7.31565C0.300269 7.43286 0.45924 7.49871 0.625 7.49871H3.125C3.29076 7.49871 3.44973 7.43286 3.56694 7.31565C3.68415 7.19844 3.75 7.03947 3.75 6.87371ZM20.625 2.49871C20.7221 2.49849 20.8178 2.4759 20.9047 2.43269L23.4047 1.18269C23.5529 1.10852 23.6657 0.978483 23.718 0.821201C23.7704 0.66392 23.7582 0.492273 23.684 0.344021C23.6473 0.270614 23.5964 0.205161 23.5344 0.151397C23.4724 0.0976336 23.4004 0.0566132 23.3225 0.0306781C23.1652 -0.0217002 22.9936 -0.00945342 22.8453 0.0647243L20.3453 1.31472C20.2194 1.37771 20.1184 1.48136 20.0588 1.60889C19.9991 1.73643 19.9843 1.88037 20.0166 2.0174C20.049 2.15442 20.1267 2.2765 20.2371 2.36386C20.3475 2.45122 20.4842 2.49873 20.625 2.49871ZM24.375 6.24871H21.875C21.7092 6.24871 21.5503 6.31456 21.4331 6.43177C21.3158 6.54898 21.25 6.70795 21.25 6.87371C21.25 7.03947 21.3158 7.19844 21.4331 7.31565C21.5503 7.43286 21.7092 7.49871 21.875 7.49871H24.375C24.5408 7.49871 24.6997 7.43286 24.8169 7.31565C24.9342 7.19844 25 7.03947 25 6.87371C25 6.70795 24.9342 6.54898 24.8169 6.43177C24.6997 6.31456 24.5408 6.24871 24.375 6.24871ZM4.65469 1.31472L2.15469 0.0647243C2.08128 0.0279952 2.00136 0.00608435 1.91948 0.00024269C1.83761 -0.00559897 1.75539 0.004743 1.67751 0.0306781C1.52023 0.0830564 1.39019 0.195769 1.31602 0.344021C1.24184 0.492273 1.22959 0.66392 1.28197 0.821201C1.33435 0.978483 1.44706 1.10852 1.59531 1.18269L4.09531 2.43269C4.18223 2.4759 4.27794 2.49849 4.375 2.49871C4.5158 2.49873 4.65248 2.45122 4.7629 2.36386C4.87332 2.2765 4.951 2.15442 4.98337 2.0174C5.01574 1.88037 5.0009 1.73643 4.94124 1.60889C4.88158 1.48136 4.78061 1.37771 4.65469 1.31472ZM23.4047 12.5647L20.9047 11.3147C20.7564 11.2405 20.5847 11.2283 20.4274 11.2807C20.2701 11.3332 20.14 11.4459 20.0658 11.5942C19.9916 11.7425 19.9794 11.9142 20.0318 12.0715C20.0842 12.2289 20.197 12.3589 20.3453 12.4331L22.8453 13.6831C22.9936 13.7573 23.1653 13.7695 23.3226 13.7171C23.4799 13.6647 23.61 13.5519 23.6842 13.4036C23.7584 13.2553 23.7706 13.0836 23.7182 12.9263C23.6658 12.769 23.553 12.6389 23.4047 12.5647V12.5647ZM4.375 11.2487C4.27794 11.2489 4.18223 11.2715 4.09531 11.3147L1.59531 12.5647C1.44701 12.6389 1.33425 12.769 1.28183 12.9263C1.25588 13.0042 1.24552 13.0864 1.25135 13.1683C1.25719 13.2502 1.27909 13.3302 1.31582 13.4036C1.35255 13.477 1.40338 13.5425 1.46542 13.5963C1.52745 13.6501 1.59947 13.6911 1.67737 13.7171C1.83469 13.7695 2.00638 13.7573 2.15469 13.6831L4.65469 12.4331C4.78083 12.3702 4.88202 12.2666 4.94183 12.1389C5.00164 12.0113 5.01656 11.8672 4.98417 11.7301C4.95178 11.5929 4.87397 11.4707 4.76339 11.3833C4.65281 11.2959 4.51594 11.2485 4.375 11.2487V11.2487Z" /></svg></p>
        <p class="dark night"><svg class="night" width="20" viewBox="0 0 25 21" fill="currentColor" xmlns="http://www.w3.org/2000/svg">
            <path class="night" d="M6.39614 3.72646C7.50591 1.56178 9.72622 0.00900831 12.4782 0.00114817C13.8006 -0.00388798 15.0965 0.375153 16.2101 1.09278C17.3237 1.8104 18.2079 2.83612 18.7564 4.04682C19.3049 5.25751 19.4945 6.60175 19.3024 7.91818C19.1103 9.23461 18.5447 10.4673 17.6735 11.4683C17.5227 11.6416 17.3516 11.859 17.1739 12.1069L9.47856 6.12184C9.65443 5.45016 10.046 4.85578 10.5924 4.43118C11.1387 4.00657 11.8093 3.77554 12.4997 3.77401C12.6654 3.77401 12.8244 3.70776 12.9416 3.58984C13.0588 3.47191 13.1247 3.31197 13.1247 3.1452C13.1247 2.97843 13.0588 2.81849 12.9416 2.70057C12.8244 2.58264 12.6654 2.51639 12.4997 2.51639C11.6212 2.5173 10.7634 2.78383 10.0374 3.28141C9.31146 3.77899 8.75092 4.48463 8.42856 5.30674L6.39614 3.72646ZM6.39614 10.0841C6.64968 10.5817 6.96225 11.0465 7.327 11.4683C7.97231 12.2091 8.98169 13.7568 9.36645 15.0624C9.36645 15.0726 9.36919 15.0828 9.37075 15.093H12.8372L6.39614 10.0841ZM9.37466 16.3502V17.8574C9.37584 18.1045 9.44934 18.3458 9.58599 18.5511L10.2536 19.5607C10.3675 19.7335 10.5221 19.8753 10.7037 19.9734C10.8852 20.0715 11.0881 20.1229 11.2942 20.1231H13.7047C13.9107 20.1231 14.1135 20.0718 14.2951 19.9739C14.4766 19.876 14.6313 19.7345 14.7454 19.5619L15.4129 18.5511C15.5492 18.3451 15.622 18.1033 15.6223 17.8558V17.2581L14.4528 16.3502H9.37466Z"/>
            <path class="night" d="M0.131556 1.2363L0.898352 0.243172C0.948738 0.177883 1.01142 0.123229 1.08282 0.0823368C1.15423 0.0414448 1.23294 0.0151171 1.31446 0.00486006C1.39598 -0.00539702 1.47872 0.000617709 1.55793 0.0225602C1.63714 0.0445026 1.71127 0.0819422 1.77609 0.132737L24.7585 18.0039C24.8894 18.1062 24.9745 18.2567 24.9952 18.4221C25.0158 18.5876 24.9703 18.7545 24.8687 18.8862L24.1015 19.8794C24.0511 19.9446 23.9884 19.9992 23.917 20.0401C23.8457 20.0809 23.767 20.1072 23.6855 20.1175C23.6041 20.1277 23.5214 20.1217 23.4422 20.0998C23.363 20.0779 23.2889 20.0405 23.2241 19.9898L0.241322 2.1186C0.110489 2.01624 0.0254259 1.86578 0.00484145 1.70032C-0.015743 1.53486 0.0298368 1.36795 0.131556 1.2363V1.2363Z"/>
            </svg></p>
        </div>
    </label>
</div>
          </li>
          <li>
            <a href="../../">
              <div class="left">
                Home
              </div>  
              <div class="right">
                <svg width="24px" aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><g><rect x="83.534" y="40.929" width="3.997" height="20.071"/></g><path d="M16.466,41.931l33.548-25.123L92.81,48.877l2.396-3.198L50.015,11.814L4.794,45.679l2.396,3.199l5.279-3.954v42.763h75.062  V61h-3.997v22.69H64.598V54.068H35.402V83.69H16.466V41.931z M39.399,58.065h21.202V83.69H39.399V58.065z"/></svg>
              </div>
            </a>
          </li>
          <li>
            <a href="../../archive.html">
              <div class="left">
                All Posts
              </div>
              <div class="right">
                <svg width="24px" aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="-3 3 64 64"><g><path d="M60.992,31.985c0-15.979-13-28.978-28.979-28.978c-15.994,0-29.006,12.999-29.006,28.978   c0,15.994,13.012,29.007,29.006,29.007v-2c-14.891,0-27.006-12.115-27.006-27.007c0-14.875,12.115-26.978,27.006-26.978   c14.876,0,26.979,12.103,26.979,26.978c0,8.945-4.479,17.329-11.804,22.338l0.874-10.062l-1.992-0.174l-1.135,13.071l13.042,1.136   l0.174-1.992l-9.183-0.799C56.443,50.079,60.992,41.321,60.992,31.985z"/><polygon points="33.014,12.682 31.014,12.682 31.014,32.398 39.811,41.224 41.227,39.812 33.014,31.572  "/></g></svg>
              </div>
            </a>
          </li>
          <li>
            <a href="../../about.html">
              <div class="left">
                About me
              </div>
              <div class="right">
                <svg width='24px' aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 846.66 846.66"><g><path d="M351.26 453.22c-276.42,134.06 -224.86,336.22 -224.73,336.8 6.03,25.41 -32.58,34.56 -38.6,9.15 -0.15,-0.65 -55.78,-219.32 218.87,-367.66 -60.98,-39 -100.02,-106.82 -100.02,-182.56 0,-119.6 96.95,-216.55 216.55,-216.55 119.6,0 216.55,96.95 216.55,216.55 0,75.74 -39.04,143.56 -100.02,182.56 274.65,148.34 219.02,367.01 218.87,367.66 -6.02,25.41 -44.63,16.26 -38.6,-9.15 0.13,-0.58 51.69,-202.74 -224.73,-336.8 -22.55,7.96 -46.8,12.29 -72.07,12.29 -25.27,0 -49.52,-4.33 -72.07,-12.29zm72.07 -381.14c-97.68,0 -176.87,79.19 -176.87,176.87 0,97.69 79.19,176.87 176.87,176.87 97.68,0 176.87,-79.18 176.87,-176.87 0,-97.68 -79.19,-176.87 -176.87,-176.87z"/></g></svg>
              </div>
            </a>
          </li>
          <li>
            <a href="../../resume.html">
              <div class="left">
                Resume
              </div>
              <div class="right">
                <!-- Add the SVG icon for Resume -->
              <svg height="24px" width="24px" viewBox="0 0 60 60" xmlns="http://www.w3.org/2000/svg" fill="currentColor">
                <g>
                  <path d="M38.914,0H6.5v60h47V14.586L38.914,0z M39.5,3.414L50.086,14H39.5V3.414z M8.5,58V2h29v14h14v42H8.5z"/>
                  <path d="M34.5,37c0.552,0,1-0.447,1-1s-0.448-1-1-1h-17c-0.552,0-1,0.447-1,1s0.448,1,1,1H34.5z"/>
                  <path d="M44.5,30h-7c-0.552,0-1,0.447-1,1s0.448,1,1,1h7c0.552,0,1-0.447,1-1S45.052,30,44.5,30z"/>
                  <path d="M21.5,31c0,0.553,0.448,1,1,1h10c0.552,0,1-0.447,1-1s-0.448-1-1-1h-10C21.948,30,21.5,30.447,21.5,31z"/>
                  <path d="M16.79,30.29c-0.18,0.189-0.29,0.449-0.29,0.71c0,0.26,0.11,0.52,0.29,0.7c0.18,0.189,0.44,0.3,0.71,0.3
                    s0.52-0.11,0.71-0.29c0.18-0.19,0.29-0.45,0.29-0.71c0-0.261-0.11-0.521-0.29-0.71C17.84,29.92,17.15,29.93,16.79,30.29z"/>
                  <path d="M38.79,35.29c-0.18,0.189-0.29,0.439-0.29,0.71c0,0.27,0.1,0.52,0.29,0.71C38.98,36.89,39.24,37,39.5,37
                    c0.26,0,0.52-0.11,0.71-0.29c0.19-0.19,0.29-0.44,0.29-0.71c0-0.261-0.11-0.521-0.29-0.71C39.83,34.92,39.17,34.92,38.79,35.29z"/>
                  <path d="M43.79,35.29c-0.18,0.189-0.29,0.439-0.29,0.71c0,0.27,0.1,0.52,0.29,0.71C43.98,36.89,44.24,37,44.5,37
                    c0.26,0,0.52-0.11,0.71-0.3c0.19-0.19,0.29-0.44,0.29-0.7c0-0.261-0.11-0.521-0.29-0.71C44.83,34.92,44.17,34.92,43.79,35.29z"/>
                  <path d="M23.025,40.166c-2.19,1.14-2.927,3.321-3.196,5.582c-0.414-0.347-0.828-0.693-1.242-1.04
                    c-0.98-0.821-2.402,0.586-1.414,1.415c0.935,0.783,1.871,1.567,2.806,2.351c0.658,0.551,1.676,0.203,1.707-0.707
                    c0.073-2.166,0.175-4.742,2.348-5.873C25.177,41.299,24.166,39.572,23.025,40.166z"/>
                  <path d="M36.455,44.108c-1.458-0.092-3.592,2.155-4.716,0.153c-0.26-0.464-0.913-0.638-1.368-0.359
                      c-1.416,0.869-3.267,2.119-4.756,0.5c-0.873-0.949-2.285,0.468-1.414,1.414c1.87,2.033,4.276,1.415,6.399,0.263
                      c0.478,0.535,1.071,0.926,1.837,1.081c0.792,0.16,4.025-1.141,4.2-0.901c0.752,1.029,2.488,0.032,1.727-1.009
                      C37.847,44.543,37.371,44.166,36.455,44.108z"/>
                  <path d="M28.666,23.963l0.674-0.479l-0.344-0.752c-0.312-0.682-0.813-1.212-1.45-1.532l-2.12-1.082
                    c0.975-0.623,1.676-1.561,2.095-2.801c0.684-0.417,1.115-1.158,1.115-1.984v-0.667c0-0.677-0.294-1.308-0.794-1.745
                    c-0.357-1.898-1.644-3.951-5.54-3.951c-0.153,0-0.303,0.006-0.451,0.018c-0.523,0.043-1.285,0-1.937-0.438
                    c-0.303-0.204-0.458-0.362-0.534-0.459c-0.317-0.403-0.849-0.544-1.324-0.35c-0.474,0.195-0.752,0.669-0.694,1.179
                    c0.03,0.257,0.073,0.557,0.138,0.884c0.084,0.42,0.089,0.541,0.086,0.573c-0.008,0.035-0.066,0.159-0.112,0.259
                    c-0.07,0.15-0.156,0.335-0.257,0.582c-0.217,0.529-0.375,1.105-0.471,1.719c-0.489,0.438-0.778,1.063-0.778,1.73v0.667
                    c0,0.826,0.431,1.567,1.115,1.984c0.417,1.235,1.115,2.171,2.083,2.793l-2.204,1.087c-0.613,0.334-1.091,0.867-1.382,1.541
                    l-0.32,0.741l0.656,0.469C17.797,25.291,20.004,26,22.302,26C24.592,26,26.792,25.296,28.666,23.963z M18.016,22.907l2.445-1.204
                    c0.519-0.257,0.842-0.776,0.842-1.355v-1.422l-0.604-0.261c-0.912-0.392-1.506-1.151-1.819-2.321l-0.143-0.533l-0.527-0.164
                    c-0.116-0.036-0.24-0.149-0.24-0.313v-0.667c0-0.142,0.095-0.242,0.184-0.289l0.469-0.25l0.055-0.529
                    c0.062-0.595,0.193-1.14,0.391-1.622c0.086-0.211,0.16-0.368,0.22-0.497c0.155-0.332,0.249-0.578,0.283-0.855
                    c0.73,0.305,1.559,0.425,2.44,0.358c0.095-0.008,0.192-0.012,0.291-0.012c2.919,0,3.469,1.334,3.622,2.638l0.061,0.523l0.466,0.245
                    c0.089,0.047,0.185,0.147,0.185,0.29v0.667c0,0.164-0.125,0.277-0.24,0.313l-0.527,0.164l-0.143,0.533
                    c-0.313,1.17-0.908,1.93-1.819,2.321l-0.604,0.261v1.428c0,0.57,0.315,1.086,0.825,1.347l2.415,1.233
                    C23.923,24.343,20.628,24.334,18.016,22.907z"/>
                  </g>
                </svg>
              </div>
            </a>
          </li>
        </ul>
      </nav>
      
      
      <div class="logo"><a href="../../"><img class="logo" id="logo" src="../../assets/img/branding/logo-full.svg" alt="Tropical Insight"></a></div>
      <div class="search-icon-container">
        <span class="search-icon"><a><i class="fa fa-search" aria-hidden="true"></i></a></span>
      </div>
    </div>
  </div>
</header> <!-- End Header -->

  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Deep Q Learning for Tic Tac Toe">
<meta itemprop="description" content="Inspired by Deep Mind’s astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe. How hard could it be?">
<meta itemprop="datePublished" content="2021-03-18T17:14:20-04:00">

    <div class="page-image">
      <div class="cover-image" style="background: url('../../assets/img/posts/20210318/TicTacToeSml.jpg') center no-repeat; background-size: cover;"></div>
    </div>
    <div class="wrapper">
      <div class="page-content">
        <div class="header-page">
          <h1 class="page-title">Deep Q Learning for Tic Tac Toe</h1>
          

  <span class = "post-page-meta">
  
    <p class="page_meta">
  
  
  
    
      
      <span class="page_meta-date">
        <time datetime="2021-03-18T17:14:20-04:00">March 18, 2021</time>
      </span>
    
    
      <span class="meta-sep"></span>
    
  
  
    
    
    <span class="page_meta-readtime">
      
        13 minute read
      
    </span>
  
  
 &nbsp;|&nbsp;Article by&nbsp;<a href="../..//about.html">Hernan Iriarte</a>
</p>
  
  </span>

        </div>
        <aside class="sidebar side" id="sidebar">
    



<div class="tag-cloud">
    
        <ul class="tags side">
            
                <li><a href="../../tag.html?tag=machine+learning" class="tag side">machine learning</a></li>
            
                <li><a href="../../tag.html?tag=artificial+intelligence" class="tag side">artificial intelligence</a></li>
            
                <li><a href="../../tag.html?tag=reinforcement+learning" class="tag side">reinforcement learning</a></li>
            
                <li><a href="../../tag.html?tag=coding" class="tag side">coding</a></li>
            
                <li><a href="../../tag.html?tag=python" class="tag side">python</a></li>
            
    
        </ul>
</div>
    <div class="share-options side">
    <div class="share-hover side">
        <span class="share-button side"><svg fill="currentColor" width="25" height="25" class="side"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></span>
        <div class="share-icons side" id="sidebar-icons">
            <a class="twitter" href="https://twitter.com/intent/tweet?text=Deep Q Learning for Tic Tac Toe&url=http://localhost:4000/Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html" title="Share on Twitter" rel="nofollow" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
            <a class="facebook" href="https://facebook.com/sharer.php?u=http://localhost:4000/Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html" title="Share on Facebook" rel="nofollow" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
            <a class="reddit" href="http://www.reddit.com/submit?url=http://localhost:4000/Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html&title=Deep Q Learning for Tic Tac Toe" title="Submit to Reddit" rel="nofollow" target="_blank"><i class="fa fa-reddit" aria-hidden="true"></i></a>
            <a class="linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html&title=Deep Q Learning for Tic Tac Toe&summary=Inspired by Deep Mind's astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe. How hard could it be?&source=http://localhost:4000/Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html" title="Share on LinkedIn" rel="nofollow" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
            <a class="email" href="mailto:?subject=Deep Q Learning for Tic Tac Toe&body=Inspired by Deep Mind's astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe. How hard could it be?%0A%0ARead more here: http://localhost:4000/Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html" title="Share via e-mail" rel="nofollow" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
            <a class="copy-link" onclick="copyToClipboard()" title="Copy to clipboard" rel="nofollow" target="_blank"><svg width="20px" fill="currentColor" class="side" viewBox="0 0 18 18"><path d="M16.94 1.1A3.7 3.7 0 0 0 14.3 0c-1 0-1.94.39-2.64 1.1L7.43 5.3c-.91.92-2.09 3.2 0 5.27a.75.75 0 0 0 .82.16c.09-.03.17-.09.24-.15a.74.74 0 0 0 0-1.06c-1.16-1.15-.77-2.39-.02-3.16l4.24-4.22a2.2 2.2 0 0 1 1.58-.65c.6 0 1.16.23 1.58.65.86.87.86 2.29 0 3.16L12.7 8.47a.74.74 0 0 0 1.04 1.05l3.17-3.16a3.73 3.73 0 0 0 0-5.27h.03zM9.54 7.4a.74.74 0 0 0 0 1.06c1.16 1.15.76 2.39 0 3.16l-4.22 4.22c-.42.42-.99.65-1.59.65a2.23 2.23 0 0 1-1.58-3.82l3.17-3.16A.73.73 0 0 0 5.54 9a.78.78 0 0 0-.22-.52.77.77 0 0 0-1.05 0L1.1 11.64A3.72 3.72 0 0 0 3.74 18c1 0 1.94-.39 2.65-1.1l4.23-4.2c.21-.22.94-1.02 1.13-2.2.18-1.12-.2-2.15-1.12-3.07-.27-.27-.78-.27-1.06 0l-.02-.02z" clip-rule="evenodd" fill-rule="evenodd"></path></svg></a>
        </div>
    </div>
    <div class='alert' style='font-size:.6em;color:var(--accent);text-align:center;'></div>
</div>
</aside>

        <aside class="toc">
  <nav class="toc-nav">
    <li class="toc-title">
      <svg aria-hidden="true" focusable="false" data-prefix="fad" data-icon="align-left" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="svg-inline--fa fa-align-left fa-w-14 fa-2x"><g class="fa-group"><path fill="currentColor" d="M12.83 352h262.34A12.82 12.82 0 0 0 288 339.17v-38.34A12.82 12.82 0 0 0 275.17 288H12.83A12.82 12.82 0 0 0 0 300.83v38.34A12.82 12.82 0 0 0 12.83 352zm0-256h262.34A12.82 12.82 0 0 0 288 83.17V44.83A12.82 12.82 0 0 0 275.17 32H12.83A12.82 12.82 0 0 0 0 44.83v38.34A12.82 12.82 0 0 0 12.83 96z" class="fa-secondary"></path><path fill="currentColor" d="M432 160H16a16 16 0 0 0-16 16v32a16 16 0 0 0 16 16h416a16 16 0 0 0 16-16v-32a16 16 0 0 0-16-16zm0 256H16a16 16 0 0 0-16 16v32a16 16 0 0 0 16 16h416a16 16 0 0 0 16-16v-32a16 16 0 0 0-16-16z" class="fa-primary"></path></g></svg>
    </li>
    <ul class="toc-content" id="toc-content"><li class="toc-item-1"><a href="#background">Background</a></li><li class="toc-item-1"><a href="#designing-the-neural-network">Designing the neural network</a></li><li class="toc-item-1"><a href="#the-many-models">The many models…</a><ul class="toc-submenu-1"><li class="toc-item-2"><a href="#model-1---the-first-try">Model 1 - the first try</a></li><li class="toc-item-2"><a href="#model-2---linear-activation-for-the-output">Model 2 - Linear activation for the output</a></li><li class="toc-item-2"><a href="#model-3---new-network-topology">Model 3 - new network topology</a></li><li class="toc-item-2"><a href="#model-4---implementing-momentum">Model 4 - implementing momentum</a></li><li class="toc-item-2"><a href="#model-5---implementing-one-hot-encoding-and-changing-topology-again">Model 5 - implementing one-hot encoding and changing topology (again)</a></li><li class="toc-item-2"><a href="#model-6---tensorflow--keras">Model 6 - Tensorflow / Keras</a></li><li class="toc-item-2"><a href="#model-7---changing-the-training-schedule">Model 7 - changing the training schedule</a></li></ul></li></ul>
  </nav>
</aside>

        
<div class="center-container">
  <div class="github-button">
    <span class="button-icon" aria-hidden="true"><svg aria-hidden="true" focusable="false" role="img" fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12,2.2467A10.00042,10.00042,0,0,0,8.83752,21.73419c.5.08752.6875-.21247.6875-.475,0-.23749-.01251-1.025-.01251-1.86249C7,19.85919,6.35,18.78423,6.15,18.22173A3.636,3.636,0,0,0,5.125,16.8092c-.35-.1875-.85-.65-.01251-.66248A2.00117,2.00117,0,0,1,6.65,17.17169a2.13742,2.13742,0,0,0,2.91248.825A2.10376,2.10376,0,0,1,10.2,16.65923c-2.225-.25-4.55-1.11254-4.55-4.9375a3.89187,3.89187,0,0,1,1.025-2.6875,3.59373,3.59373,0,0,1,.1-2.65s.83747-.26251,2.75,1.025a9.42747,9.42747,0,0,1,5,0c1.91248-1.3,2.75-1.025,2.75-1.025a3.59323,3.59323,0,0,1,.1,2.65,3.869,3.869,0,0,1,1.025,2.6875c0,3.83747-2.33752,4.6875-4.5625,4.9375a2.36814,2.36814,0,0,1,.675,1.85c0,1.33752-.01251,2.41248-.01251,2.75,0,.26251.1875.575.6875.475A10.0053,10.0053,0,0,0,12,2.2467Z"/></svg></span>
    <a class="button-text" href="https://github.com/amaynez/TicTacToe/" target="_blank" aria-label="Open on GitHub">view on <b>GitHub</b></a>
  </div>
</div>

        <center><img style="float: left;margin-right: 1em;" src="./assets/img/posts/20210318/Game_Screen.png" width="310" height="300" /></center>

<h2 id="background">Background</h2>
<p>After many years of a corporate career (17) diverging from computer science, I have now decided to learn Machine Learning and in the process return to coding (something I have always loved!).</p>

<p>To fully grasp the essence of ML I decided to start by <a href="./ML-Library-from-scratch.html">coding a ML library myself</a>, so I can fully understand the inner workings, linear algebra and calculus involved in Stochastic Gradient Descent. And on top learn Python (I used to code in C++ 20 years ago).</p>

<p>I built a general purpose basic ML library that creates a Neural Network (only DENSE layers), saves and loads the weights into a file, does forward propagation and training (optimization of weights and biases) using SGD. I tested the ML library with the XOR problem to make sure it worked fine. You can read the blog post for it <a href="./ML-Library-from-scratch.html">here</a>.</p>

<p>For the next challenge I am interested in reinforcement learning greatly inspired by Deep Mind’s astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe (or noughts and crosses).</p>

<p>How hard could it be?</p>

<p>Of course the first thing to do was to program the game itself, so I chose Python because I am learning it, so it gives me a good practice opportunity, and PyGame for the interface.
Coding the game was quite straightforward, albeit for the hiccups of being my first PyGame and almost my first Python program ever.
I created the game quite openly, in such a way that it can be played by two humans, by a human vs. an algorithmic AI, and a human vs. the neural network. And of course the neural network against a choice of 3 AI engines: random, <a href="https://en.wikipedia.org/wiki/Minimax">minimax</a> or hardcoded (an exercise I wanted to do since a long time).</p>

<p>While training, the visuals of the game can be disabled to make training much faster.
Now, for the fun part, training the network, I followed Deep Mind’s own DQN recommendations:</p>

<ul><li>The network will be an approximation for the Q value function or Bellman equation, meaning that the network will be trained to predict the "value" of each move available in a given game state.</li><li>A replay experience memory was implemented. This meant that the neural network will not be trained after each move. Each move will be recorded in a special "memory" alongside with the state of the board and the reward it received for taking such an action (move).</li><li>After the memory is sizable enough, batches of random experiences sampled from the replay memory are used for every training round</li><li>A secondary neural network (identical to the main one) is used to calculate part of the Q value function (Bellman equation), in particular the future Q values. And then it is updated with the main network's weights every <em>n</em> games. This is done so that we are not chasing a moving target.</li></ul>

<h2 id="designing-the-neural-network">Designing the neural network</h2>

<center><img src="./assets/img/posts/20210318/Neural_Network_Topology.png" width="540" /></center>
<p><br /></p>

<p>The Neural Network chosen takes 9 inputs (the current state of the game) and outputs 9 Q values for each of the 9 squares in the board of the game (possible actions). Obviously some squares are illegal moves, hence while training there was a negative reward given to illegal moves hoping that the model would learn not to play illegal moves in a given position.</p>

<p>I started out with two hidden layers of 36 neurons each, all fully connected and activated via ReLu. The output layer was initially activated using sigmoid to ensure that we get a nice value between 0 and 1 that represents the QValue of a given state action pair.</p>

<h2 id="the-many-models">The many models…</h2>
<h3 id="model-1---the-first-try">Model 1 - the first try</h3>

<p>At first the model was trained by playing vs. a “perfect” AI, meaning a <a href="https://github.com/amaynez/TicTacToe/blob/b429e5637fe5f61e997f04c01422ad0342565640/entities/Game.py#L43">hard coded algorithm</a> that never looses and that will win if it is given the chance. After several thousand training rounds, I noticed that the Neural Network was not learning much; so I switched to training vs. a completely random player, so that it will also learn how to win. After training vs. the random player, the Neural Network seems to have made progress and is steadily diminishing the loss function over time.</p>

<center><img src="./assets/img/posts/20210318/Loss_function_across_all_episodes.png" width="540" /></center>
<p><br /></p>

<p>However, the model was still generating many illegal moves, so I decided to modify the reinforcement learning algorithm to punish more the illegal moves. The change consisted in populating with zeros all the corresponding illegal moves for a given position at the target values to train the network. This seemed to work very well for diminishing the illegal moves:</p>

<center><img src="./assets/img/posts/20210318/Loss_function_and_Illegal_moves.png" width="540" /></center>
<p><br /></p>

<p>Nevertheless, the model was still performing quite poorly winning only around 50% of games vs. a completely random player (I expected it to win above 90% of the time). This was after only training 100,000 games, so I decided to keep training and see the results:</p>

<center><img src="./assets/img/posts/20210318/Loss_function_and_Illegal_moves2.png" width="540" />
<small>Wins: 65.46% Losses: 30.32% Ties: 4.23%</small></center>

<p>Note that when training restarts, the loss and illegal moves are still high in the beginning of the training round, and this is caused by the epsilon greedy strategy that prefers exploration (a completely random move) over exploitation, this preference diminishes over time.</p>

<p>After another round of 100,000 games, I can see that the loss function actually started to diminish, and the win rate ended up at 65%, so with little hope I decided to carry on and do another round of 100,000 games (about 2 hours in an i7 MacBook Pro):</p>

<center><img src="./assets/img/posts/20210318/Loss_function_and_Illegal_moves3.png" width="540" />
<small>Wins: 46.40% Losses: 41.33% Ties: 12.27%</small></center>

<p>As you can see in the chart, the calculated loss not even plateaued, but it seemed to increase a bit over time, which tells me the model is not learning anymore. This was confirmed by the win rate decreasing with respect of the previous round to a meek 46.4% that looks no better than a random player.</p>

<h3 id="model-2---linear-activation-for-the-output">Model 2 - Linear activation for the output</h3>

<p>After not getting the results I wanted, I decided to change the output activation function to linear, since the output is supposed to be a Q value, and not a probability of an action.</p>

<center><img src="./assets/img/posts/20210318/Loss_function_and_Illegal_moves4.png" width="540" /><br />
<small>Wins: 47.60% Losses: 39% Ties: 13.4%</small></center>
<p><br /></p>

<p>Initially I tested with only 1000 games to see if the new activation function was working, the loss function appears to be decreasing, however it reached a plateau around a value of 1, hence still not learning as expected. I came across a <a href="https://github.com/bckenstler/CLR">technique by Brad Kenstler, Carl Thome and Jeremy Jordan</a> called Cyclical Learning Rate, which appears to solve some cases of stagnating loss functions in this type of networks. So I gave it a go using their Triangle 1 model.</p>

<p>With the cycling learning rate in place, still no luck after a quick 1,000 games training round; so I decided to implement on top a decaying learning rate as per the following formula:</p>

<center><img src="./assets/img/posts/20210318/lr_formula.jpeg" width="280" /></center>

<p>The resulting learning rate combining the cycles and decay per epoch is:</p>
<center><img src="./assets/img/posts/20210318/LR_cycle_decay.png" width="480" />
<small>Learning Rate = 0.1, Decay = 0.0001, Cycle = 2048 epochs,<br />
        max Learning Rate factor = 10x</small></center>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">true_epoch</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BATCH_SIZE</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">c</span><span class="p">.</span><span class="n">DECAY_RATE</span><span class="o">*</span><span class="n">true_epoch</span><span class="p">))</span>
<span class="k">if</span> <span class="n">c</span><span class="p">.</span><span class="n">CLR_ON</span><span class="p">:</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">cyclic_learning_rate</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">true_epoch</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">cyclic_learning_rate</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
    <span class="n">max_lr</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">c</span><span class="p">.</span><span class="n">MAX_LR_FACTOR</span>
    <span class="n">cycle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">floor</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="p">(</span><span class="n">epoch</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">c</span><span class="p">.</span><span class="n">LR_STEP_SIZE</span><span class="p">)))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">((</span><span class="n">epoch</span><span class="o">/</span><span class="n">c</span><span class="p">.</span><span class="n">LR_STEP_SIZE</span><span class="p">)</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">cycle</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">learning_rate</span><span class="o">+</span><span class="p">(</span><span class="n">max_lr</span><span class="o">-</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c</span><span class="p">.</span><span class="n">DECAY_RATE</span> <span class="o">=</span> <span class="n">learning</span> <span class="n">rate</span> <span class="n">decay</span> <span class="n">rate</span>
<span class="n">c</span><span class="p">.</span><span class="n">MAX_LR_FACTOR</span> <span class="o">=</span> <span class="n">multiplier</span> <span class="n">that</span> <span class="n">determines</span> <span class="n">the</span> <span class="nb">max</span> <span class="n">learning</span> <span class="n">rate</span>
<span class="n">c</span><span class="p">.</span><span class="n">LR_STEP_SIZE</span> <span class="o">=</span> <span class="n">the</span> <span class="n">number</span> <span class="n">of</span> <span class="n">epochs</span> <span class="n">each</span> <span class="n">cycle</span> <span class="n">lasts</span>
</code></pre></div></div>
<p><br />With these many changes, I decided to restart with a fresh set of random weights and biases and try training more (much more) games.</p>

<center><img src="./assets/img/posts/20210318/Loss_function_and_Illegal_moves6.png" width="540" />
<small>1,000,000 episodes, 7.5 million epochs with batches of 64 moves each<br />
Wins: 52.66% Losses: 36.02% Ties: 11.32%</small></center>

<p>After <strong>24 hours!</strong>, my computer was able to run 1,000,000 episodes (games played), which represented 7.5 million training epochs of batches of 64 plays (480 million plays learned), the learning rate did decreased (a bit), but is clearly still in a plateau; interestingly, the lower boundary of the loss function plot seems to continue to decrease as the upper bound and the moving average remains constant. This led me to believe that I might have hit a local minimum.
<a name="Model3"></a></p>
<h3 id="model-3---new-network-topology">Model 3 - new network topology</h3>

<p>After all the failures I figured I had to rethink the topology of the network and play around with combinations of different networks and learning rates.</p>

<center><img src="./assets/img/posts/20210318/Loss_function_and_Illegal_moves7.png" width="540" />
<small>100,000 episodes, 635,000 epochs with batches of 64 moves each<br />
<b>Wins: 76.83%</b> Losses: 17.35% Ties: 5.82%</small></center>

<p>I increased to 200 neurons each hidden layer. In spite of this great improvement the loss function was still in a plateau at around 0.1 (Mean Squared Error). Which, although it is greatly reduced from what we had, still was giving out only 77% win rate vs. a random player, the network was playing tic tac toe as a toddler!</p>

<center><img src="./assets/img/posts/20210318/Game_Screen2.png" width="240" height="240" />
<small>*I can still beat the network most of the time! (I am playing with the red X)*</small></center>

<center><img src="./assets/img/posts/20210318/Loss_function_and_Illegal_moves10.png" width="540" />
<small>100,000 more episodes, 620,000 epochs with batches of 64 moves each<br />
<b>Wins: 82.25%</b> Losses: 13.28% Ties: 4.46%</small></center>

<p><strong>Finally we crossed the 80% mark!</strong> This is quite an achievement, it seems that the change in network topology is working, although it also looks like the loss function is stagnating at around 0.15.</p>

<p>After more training rounds and some experimenting with the learning rate and other parameters, I couldn’t improve past the 82.25% win rate.</p>

<p>These have been the results so far:</p>

<center><img src="./assets/img/posts/20210318/Models1to3.png" width="540" /></center>
<p><br /></p>

<p>It is quite interesting to learn how the many parameters (hyper-parameters as most authors call them) of a neural network model affect its training performance, I have played with:</p>
<ul>
  <li>the learning rate</li>
  <li>the network topology and activation functions</li>
  <li>the cycling and decaying learning rate parameters</li>
  <li>the batch size</li>
  <li>the target update cycle (when the target network is updated with the weights from the policy network)</li>
  <li>the rewards policy</li>
  <li>the epsilon greedy strategy</li>
  <li>whether to train vs. a random player or an “intelligent” AI.</li>
</ul>

<p>And so far the most effective change has been the network topology, but being so close but not quite there yet to my goal of 90% win rate vs. a random player, I will still try to optimize further.</p>

<tweet>Network topology seems to have the biggest impact on a neural network's learning ability.</tweet>

<p><a name="Model4"></a></p>
<h3 id="model-4---implementing-momentum">Model 4 - implementing momentum</h3>

<p>I <a href="https://www.reddit.com/r/MachineLearning/comments/lzvrwp/p_help_with_a_reinforcement_learning_project/">reached out to the reddit community</a> and a kind soul pointed out that maybe what I need is to apply momentum to the optimization algorithm. So I did some research and ended up deciding to implement various optimization methods to experiment with:</p>

<ul>
  <li>Stochastic Gradient Descent with Momentum</li>
  <li>RMSProp: Root Mean Square Plain Momentum</li>
  <li>NAG: Nezterov’s Accelerated Momentum</li>
  <li>Adam: Adaptive Moment Estimation</li>
  <li>and keep my old vanilla Gradient Descent (vGD) ☺</li>
</ul>

<p><a name="optimization"></a><a href="https://the-mvm.github.io/neural-network-optimization-methods/">Click here for a detailed explanation and code of all the implemented optimization algorithms.</a></p>

<p>So far, I have not been able to get better results with Model 4, I have tried all the momentum optimization algorithms with little to no success.
<a name="Model5"></a></p>
<h3 id="model-5---implementing-one-hot-encoding-and-changing-topology-again">Model 5 - implementing one-hot encoding and changing topology (again)</h3>
<p>I came across an <a href="https://github.com/AxiomaticUncertainty/Deep-Q-Learning-for-Tic-Tac-Toe/blob/master/tic_tac_toe.py">interesting project in Github</a> that deals exactly with Deep Q Learning, and I noticed that he used “one-hot” encoding for the input as opposed to directly entering the values of the player into the 9 input slots. So I decided to give it a try and at the same time change my topology to match his:</p>

<center><img src="./assets/img/posts/20210318/Neural_Network_Topology3.png" width="540" /></center>

<p>So, ‘one hot’ encoding is basically changing the input of a single square in the tic tac toe board to three numbers, so that each state is represented with different inputs, thus the network can clearly differentiate the three of them. As the original author puts it, the way I was encoding, having 0 for empty, 1 for X and 2 for O, the network couldn’t easily tell that, for instance, O and X both meant occupied states, because one is two times as far from 0 as the other. With the new encoding, the empty state will be 3 inputs: (1,0,0), the X will be (0,1,0) and the O (0,0,1) as in the diagram.</p>

<p>Still, no luck even with Model 5, so I am starting to think that there could be a bug in my code.</p>

<p>To test this hypothesis, I decided to implement the same model using Tensorflow / Keras.</p>

<p><a name="Model6"></a></p>
<h3 id="model-6---tensorflow--keras">Model 6 - Tensorflow / Keras</h3>
<center><img src="https://www.kubeflow.org/docs/images/logos/TensorFlow.png" width="100" height="100" /></center>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">PolicyNetwork</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">()</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">hidden_layers</span><span class="p">:</span>
    <span class="n">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span>
                           <span class="n">units</span><span class="o">=</span><span class="n">layer</span><span class="p">,</span>
                           <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span>
                           <span class="n">input_dim</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                           <span class="n">kernel_initializer</span><span class="o">=</span><span class="sh">'</span><span class="s">random_uniform</span><span class="sh">'</span><span class="p">,</span>
                           <span class="n">bias_initializer</span><span class="o">=</span><span class="sh">'</span><span class="s">zeros</span><span class="sh">'</span><span class="p">))</span>
<span class="n">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span>
                        <span class="n">outputs</span><span class="p">,</span>
                        <span class="n">kernel_initializer</span><span class="o">=</span><span class="sh">'</span><span class="s">random_uniform</span><span class="sh">'</span><span class="p">,</span>
                        <span class="n">bias_initializer</span><span class="o">=</span><span class="sh">'</span><span class="s">zeros</span><span class="sh">'</span><span class="p">))</span>
<span class="n">opt</span> <span class="o">=</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">LEARNING_RATE</span><span class="p">,</span>
           <span class="n">beta_1</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">GAMMA_OPT</span><span class="p">,</span>
           <span class="n">beta_2</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">BETA</span><span class="p">,</span>
           <span class="n">epsilon</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">,</span>
           <span class="n">amsgrad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span>
                           <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">mean_squared_error</span><span class="sh">'</span><span class="p">,</span>
                           <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>
</code></pre></div></div>
<p>As you can see I am reusing all of my old code, and just replacing my Neural Net library with Tensorflow/Keras, keeping even my hyper-parameter constants.</p>

<p>The training function changed to:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">reduce_lr_on_plateau</span> <span class="o">=</span> <span class="nc">ReduceLROnPlateau</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">,</span>
                                         <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                         <span class="n">patience</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">states_to_train</span><span class="p">),</span>
                                 <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">targets_to_train</span><span class="p">),</span>
                                 <span class="n">epochs</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">EPOCHS</span><span class="p">,</span>
                                 <span class="n">batch_size</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                 <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                 <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">reduce_lr_on_plateau</span><span class="p">],</span>
                                 <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>With Tensorflow implemented, the first thing I noticed, was that I had an error in the calculation of the loss, although this only affected reporting and didn’t change a thing on the training of the network, so the results kept being the same, <strong>the loss function was still stagnating! My code was not the issue.</strong>
<a name="Model7"></a></p>
<h3 id="model-7---changing-the-training-schedule">Model 7 - changing the training schedule</h3>
<p>Next I tried to change the way the network was training as per <a href="https://www.reddit.com/user/elBarto015">u/elBarto015</a> <a href="https://www.reddit.com/r/reinforcementlearning/comments/lzzjar/i_created_an_ai_for_super_hexagon_based_on/gqc8ka6?utm_source=share&amp;utm_medium=web2x&amp;context=3">advised me on reddit</a>.</p>

<p>The way I was training initially was:</p>
<ul>
  <li>Games begin being simulated and the outcome recorded in the replay memory</li>
  <li>Once a sufficient ammount of experiences are recorded (at least equal to the batch size) the Network will train with a random sample of experiences from the replay memory. The ammount of experiences to sample is the batch size.</li>
  <li>The games continue to be played between the random player and the network.</li>
  <li>Every move from either player generates a new training round, again with a random sample from the replay memory.</li>
  <li>This continues until the number of games set up conclude.</li>
</ul>

<center><img src="./assets/img/posts/20210318/ReplayMemoryBefore.png" width="540" /></center>

<p>The first change was to train only after every game concludes with the same ammount of data (a batch). This was still not giving any good results.</p>

<p>The second change was more drastic, it introduced the concept of epochs for every training round, it basically sampled the replay memory for epochs * batch size experiences, for instance if epochs selected were 10, and batch size was 81, then 810 experiences were sampled out of the replay memory. With this sample the network was then trained for 10 epochs randomly using the batch size.</p>

<p>This meant that I was training now effectively 10 (or the number of epochs selected) times more per game, but in batches of the same size and randomly shuffling the experiences each epoch.</p>

<center><img src="./assets/img/posts/20210318/ReplayMemoryAfter.png" width="540" /></center>
<p><br /></p>

<p>After still playing around with some hyperparameters I managed to get similar performance as I got before, reaching 83.15% win rate vs. the random player, so I decided to keep training in rounds of 2,000 games each to evaluate performance. With almost every round I could see improvement:</p>

<center><img src="./assets/img/posts/20210318/Model7HyperParameters.png" width="540" /><br />
<img src="./assets/img/posts/20210318/Model7.png" width="480" />
</center>
<p><br /></p>

<p>As of today, my best result so far is 87.5%, I will leave it rest for a while and keep investigating to find a reason for not being able to reach at least 90%. I read about <a href="https://medium.com/applied-data-science/how-to-train-ai-agents-to-play-multiplayer-games-using-self-play-deep-reinforcement-learning-247d0b440717">self play</a>, and it looks like a viable option to test and a fun coding challenge. However, before embarking in yet another big change I want to ensure I have been thorough with the model and have tested every option correctly.</p>

<p>I feel the end is near… should I continue to update this post as new events unfold or shall I make it a multi post thread?</p>

        <aside class="sidebar inline" id="post-end">
    



<div class="tag-cloud">
    
        <ul class="tags inline">
            
                <li><a href="../../tag.html?tag=machine+learning" class="tag inline">machine learning</a></li>
            
                <li><a href="../../tag.html?tag=artificial+intelligence" class="tag inline">artificial intelligence</a></li>
            
                <li><a href="../../tag.html?tag=reinforcement+learning" class="tag inline">reinforcement learning</a></li>
            
                <li><a href="../../tag.html?tag=coding" class="tag inline">coding</a></li>
            
                <li><a href="../../tag.html?tag=python" class="tag inline">python</a></li>
            
    
        </ul>
</div>
    <div class="share-options inline">
    <div class="share-hover inline">
        <span class="share-button inline"><svg fill="currentColor" width="25" height="25" class="inline"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></span>
        <div class="share-icons inline" id="post-end-icons">
            <a class="twitter" href="https://twitter.com/intent/tweet?text=Deep Q Learning for Tic Tac Toe&url=http://localhost:4000/Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html" title="Share on Twitter" rel="nofollow" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
            <a class="facebook" href="https://facebook.com/sharer.php?u=http://localhost:4000/Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html" title="Share on Facebook" rel="nofollow" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
            <a class="reddit" href="http://www.reddit.com/submit?url=http://localhost:4000/Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html&title=Deep Q Learning for Tic Tac Toe" title="Submit to Reddit" rel="nofollow" target="_blank"><i class="fa fa-reddit" aria-hidden="true"></i></a>
            <a class="linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html&title=Deep Q Learning for Tic Tac Toe&summary=Inspired by Deep Mind's astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe. How hard could it be?&source=http://localhost:4000/Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html" title="Share on LinkedIn" rel="nofollow" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
            <a class="email" href="mailto:?subject=Deep Q Learning for Tic Tac Toe&body=Inspired by Deep Mind's astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe. How hard could it be?%0A%0ARead more here: http://localhost:4000/Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html" title="Share via e-mail" rel="nofollow" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
            <a class="copy-link" onclick="copyToClipboard()" title="Copy to clipboard" rel="nofollow" target="_blank"><svg width="20px" fill="currentColor" class="inline" viewBox="0 0 18 18"><path d="M16.94 1.1A3.7 3.7 0 0 0 14.3 0c-1 0-1.94.39-2.64 1.1L7.43 5.3c-.91.92-2.09 3.2 0 5.27a.75.75 0 0 0 .82.16c.09-.03.17-.09.24-.15a.74.74 0 0 0 0-1.06c-1.16-1.15-.77-2.39-.02-3.16l4.24-4.22a2.2 2.2 0 0 1 1.58-.65c.6 0 1.16.23 1.58.65.86.87.86 2.29 0 3.16L12.7 8.47a.74.74 0 0 0 1.04 1.05l3.17-3.16a3.73 3.73 0 0 0 0-5.27h.03zM9.54 7.4a.74.74 0 0 0 0 1.06c1.16 1.15.76 2.39 0 3.16l-4.22 4.22c-.42.42-.99.65-1.59.65a2.23 2.23 0 0 1-1.58-3.82l3.17-3.16A.73.73 0 0 0 5.54 9a.78.78 0 0 0-.22-.52.77.77 0 0 0-1.05 0L1.1 11.64A3.72 3.72 0 0 0 3.74 18c1 0 1.94-.39 2.65-1.1l4.23-4.2c.21-.22.94-1.02 1.13-2.2.18-1.12-.2-2.15-1.12-3.07-.27-.27-.78-.27-1.06 0l-.02-.02z" clip-rule="evenodd" fill-rule="evenodd"></path></svg></a>
        </div>
    </div>
    <div class='alert' style='font-size:.6em;color:var(--accent);text-align:center;'></div>
</div>
</aside>

        <div class="separator"></div>
        



<div class="recent-box">
  <h2 class="page-subtitle">Recent posts</h2>
  <div class="recent-list">
    
  </div>
</div> <!-- End Recent-Box -->

      </div>
    </div> <!-- End Wrapper -->
  </article>
  <div class="search-box">
  <div class="wrapper">
    <div class="search-grid">
      <form class="search-form">
        <div id="search-container">
          <input type="text" id="search-input" class="search" placeholder="Search">
        </div>
      </form>
      <ul id="results-container" class="results-search"></ul>
      <div class="icon-close-container">
        <span class="search-icon-close"><i class="fa fa-times" aria-hidden="true"></i></span>
      </div>
    </div>
  </div>
</div>

  




<footer class="main-footer">
    <div class="footer-wrapper">
        <div class="logo-symbol">
            <a class="logo-link" title="Tropical Insight" href="../../">
<svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
     width="45px" height="45px" viewBox="0 0 283.46 283.46" xml:space="preserve">
    <g id="Circle">
        <circle fill="none" stroke="#000000" stroke-width="7" stroke-miterlimit="10" cx="141.73" cy="141.73" r="137.501"/>
    </g>
    <g id="Palm_tree">
        <g id="Stem">
            <path fill="#CA9E67" d="M123.059,103.333c24.271,71.429-13.726,156.866-13.726,156.866l47.714,6.396
                c0,0,30.719-110.32,5.229-159.34C156.072,95.324,147.568,95.199,123.059,103.333z"/>
            <path fill="#B17F4A" d="M131.128,124.546c19.709-11.265,21.713,4.12,35.372-0.341l4.861,26.84c0,0-31.309-16.781-39.826,0.34
                l1.655,23.008c12.88-12.255,25.818,10.134,37.38,2.568c0.52,7.122,2.523,22.543-2.908,26.335
                c-8.436,5.891-23.561-14.524-40.673-4.367c-1.837,9.003-6.754,21.405-3.068,18.507c10.894-8.565,30.175,10.716,41.324,1.871
                c5.391-4.278,0.207,22.312-4.713,27.123c-8.715,8.519-27.341-14.03-43.691-5.534C137.429,199.719,136.357,157.553,131.128,124.546
                z"/>
        </g>
        <g id="leaves">
            <path fill="#3AAA35" d="M133.19,64.449c10.912-8.17,25.576-1.329,43.464,6.535c27.123,5.882,56.535,8.169,62.092,90.522
                c-25.92-30.282-52.082-50.452-79.469-57.239c-13.67,5.275-22.167-5.183-33.001,4.306c-35.294,11.438-52.237,30.711-65.636,79.077
                c-14.38-24.739-21.896-74.184,22.222-101.634C95.751,72.912,117.784,61.522,133.19,64.449z"/>
            <path fill="#006633" d="M27.635,92.23c0,0,43.79-76.475,102.613-28.436c40.85-74.023,100.328-9.146,100.328-9.146
                s-39.545-9.484-55.883,18.62c-13.072-7.193-33.742-10.458-42.811-3.922c-15.36-3.595-34.968,7.513-47.059,18.627
                C65.869,71.96,27.635,92.23,27.635,92.23z"/>
        </g>
        <g id="Contour">
            <path fill="none" stroke="#000000" stroke-width="7" stroke-miterlimit="10" d="M27.635,92.23c0,0,43.79-76.475,102.613-28.436
                c40.85-74.023,100.328-9.146,100.328-9.146s-37.584-11.769-53.922,16.336c27.123,5.882,56.535,8.169,62.092,90.522
                c-25.92-30.282-52.082-50.452-79.469-57.239c10.783,23.856,22.279,63.634-2.23,162.328c-14.788-3.618-28.433-5.088-47.714-6.396
                c34.313-71.569,23.284-142.529,16.943-151.626c-35.294,11.438-52.237,30.711-65.636,79.077
                c-14.38-24.739-21.896-74.184,22.222-101.634C63.908,70.003,27.635,92.23,27.635,92.23z"/>
        </g>
    </g>
</svg>

            </a>
        </div>
        <div class="copyright">
          <p>2024 &copy; Hernan Iriarte</p>
        </div>
        <div class="footer-nav">
            <div>
                <a href="../../archive.html">
                    Posts
                </a>
            </div>
            <div>
                <a href="../../tags.html">
                    Tags
                </a>
            </div>
            <div>
                <a href="../../about.html">
                    About me
                </a>
            </div>
        </div>
    </div>
</footer> <!-- End Footer -->

</div>

    <div class="top" title="Top">
      <svg aria-hidden="true" focusable="false" data-prefix="fal" data-icon="angle-up" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="svg-inline--fa fa-angle-up fa-w-8 fa-2x"><path fill="currentColor" d="M136.5 185.1l116 117.8c4.7 4.7 4.7 12.3 0 17l-7.1 7.1c-4.7 4.7-12.3 4.7-17 0L128 224.7 27.6 326.9c-4.7 4.7-12.3 4.7-17 0l-7.1-7.1c-4.7-4.7-4.7-12.3 0-17l116-117.8c4.7-4.6 12.3-4.6 17 .1z" class=""></path></svg>
    </div>
    




<!-- JS -->








<script>
(function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var logo = document.getElementById('logo');
    var nightModeOption = ('manual' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
    storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
    var data = storage.getItem('theme');
    try {
        data = JSON.parse(data ? data : '');
    } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
    }
    return data;
    }

    function handleThemeToggle(nightShift) {
    themeData.nightShift = nightShift;
    saveThemeData(themeData);
    html.dataset.theme = nightShift ? 'dark' : 'light';
    if (nightShift) {
        logo.setAttribute("src", "../../assets/img/branding/logo-full-dark.svg");
    } else {
        logo.setAttribute("src", "../../assets/img/branding/logo-full.svg");
    }
    setTimeout(function() {
        sw.checked = nightShift ? true : false;
    }, 50);
    }

    function autoThemeToggle() {
    // Next time point of theme toggle
    var now = new Date();
    var toggleAt = new Date();
    var hours = now.getHours();
    var nightShift = hours >= 19 || hours <=7;

    if (nightShift) {
        if (hours > 7) {
        toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
    } else {
        toggleAt.setHours(19);
    }

    toggleAt.setMinutes(0);
    toggleAt.setSeconds(0);
    toggleAt.setMilliseconds(0)

    var delay = toggleAt.getTime() - now.getTime();

    // auto toggle theme mode
    setTimeout(function() {
        handleThemeToggle(!nightShift);
    }, delay);

    return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
    };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
    handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
    var data = autoThemeToggle();

    // Toggle theme by local setting
    if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
    } else {
        handleThemeToggle(themeData.nightShift);
    }
    } else if (nightModeOption == 'manual') {
    handleThemeToggle(themeData.nightShift);
    } else {
    var nightShift = themeData.nightShift;
    if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
    }
    handleThemeToggle(nightShift);
    }
})();
</script>

<script src="../../assets/js/jekyll-search.js"></script>
<script src="../../assets/js/jquery-3.6.0.min.js"></script>


  <script>
    function toggle_comments(){
      var commentCurtain = document.getElementById('comment-curtain')
      if (commentCurtain) {
        commentCurtain.classList.toggle('hide')
      }
      var disqusThread = document.getElementById('comment-layout')
      if (disqusThread) {
        disqusThread.classList.toggle('show')
      }
    }

    function copyToClipboard() {
      navigator.clipboard.writeText('http://localhost:4000/Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html').then(function() {
      alerts = document.getElementsByClassName('alert')
      for (i=0; i < alerts.length; i++){
        alerts[i].innerHTML='\u00ABlink copied\u00BB';
        setTimeout((function(i){ return function(){alerts[i].innerHTML='';}})(i), 1600 );
      };
      }, function() {
        prompt("Unable to copy, please use this link:", "http://localhost:4000/Legacy/Posts/2021-03-18-deep-q-learning-tic-tac-toe.html");
      });
    }

    $(function () {
      if (document.getElementById('comment-curtain') == null){
        var disqusThread = document.getElementById('comment-layout')
        if (disqusThread) {
          disqusThread.classList.toggle('show')
        }
      }

      var tweetTags = document.getElementsByTagName("tweet");

      if (tweetTags != null){
        for (i=0; i<tweetTags.length; i++){
          tweetA = document.createElement("a")
          tweetA.href = 'https://twitter.com/share?text='
                       + encodeURIComponent(tweetTags[i].textContent)
                       + '&via=&url='
                       + window.location.href;
          tweetA.target = "_blank";
          tweetA.className = 'twitter';
          tweetSpanText = document.createElement('span');
          tweetSpanText.className = 'tweetText';
          tweetSpanText.appendChild(document.createTextNode(tweetTags[i].textContent));
          tweetSpanIcon = document.createElement('span');
          tweetSpanIcon.className = 'tweetIcon';
          tweetSpanIcon.appendChild(document.createTextNode("click to tweet"));
          tweetI = document.createElement("i");
          tweetI.className = 'fa fa-twitter';
          tweetSpanIcon.appendChild(tweetI);
          tweetA.appendChild(tweetSpanText);
          tweetA.appendChild(tweetSpanIcon);
          tweetTags[i].textContent = "";
          tweetTags[i].appendChild(tweetA);
        }
      }

    });

  </script>
  <!-- Mailchimp linking -->
  <script id="mcjs">!function(c,h,i,m,p){m=c.createElement(h),p=c.getElementsByTagName(h)[0],m.async=1,m.src=i,p.parentNode.insertBefore(m,p)}(document,"script","https://chimpstatic.com/mcjs-connected/js/users/8ece198b3eb260e6838461a60/d20d9fb9aad962399025da52e.js");</script>



  <script src="https://cdn.jsdelivr.net/gh/cferdinandi/gumshoe@5.1.1/dist/gumshoe.polyfills.min.js"></script>
  <script>
    var spy = new Gumshoe("#toc-content a", {
      navClass:"active",
      contentClass:"underline",
      nested:0,
      nestedClass:"active",
      offset:20,
      reflow:1,
      events:1
    });

    var coll = document.getElementsByClassName("toc-item-1");
    var i;
    var chevron_up = "<svg aria-hidden=\"true\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 256 512\"><path fill=\"currentColor\" d=\"M136.5 185.1l116 117.8c4.7 4.7 4.7 12.3 0 17l-7.1 7.1c-4.7 4.7-12.3 4.7-17 0L128 224.7 27.6 326.9c-4.7 4.7-12.3 4.7-17 0l-7.1-7.1c-4.7-4.7-4.7-12.3 0-17l116-117.8c4.7-4.6 12.3-4.6 17 .1z\"></path></svg>"
    var chevron_down = "<svg aria-hidden=\"true\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 256 512\"><path fill=\"currentColor\" d=\"M119.5 326.9L3.5 209.1c-4.7-4.7-4.7-12.3 0-17l7.1-7.1c4.7-4.7 12.3-4.7 17 0L128 287.3l100.4-102.2c4.7-4.7 12.3-4.7 17 0l7.1 7.1c4.7 4.7 4.7 12.3 0 17L136.5 327c-4.7 4.6-12.3 4.6-17-.1z\"></path></svg>"
    for (i = 0; i < coll.length; i++) {
      if (coll[i].childElementCount > 1) {
        sign = document.createElement('div');
        sign.className = "toc-sign";
        sign.innerHTML = chevron_down;
        coll[i].insertBefore(sign, coll[i].childNodes[0].nextSibling);
        coll[i].addEventListener("click", function() {
          var content = this.lastElementChild;
          if (content.style.maxHeight){
            content.style.maxHeight = null;
            this.firstElementChild.nextSibling.innerHTML = chevron_down;
          } else {
            content.style.maxHeight = content.scrollHeight + "px";
            this.firstElementChild.nextSibling.innerHTML = chevron_up;
          }
        });
      }
    }
  </script>




<script src="../../assets/js/main.js"></script>
<script>
  SimpleJekyllSearch({
      searchInput: document.getElementById('search-input'),
      resultsContainer: document.getElementById('results-container'),
      json: '../../search.json',
      searchResultTemplate: '<li><a href="{url}" title="{description}">{title}</a><p>{description}</p></li>',
      noResultsText: 'No results found',
      fuzzy: false,
      exclude: ['Welcome']
    });
</script>




    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=GTM-KJZ983L4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'GTM-KJZ983L4');
</script>
  </body>
</html>
